{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd964940-06b5-4467-b2db-0d92718820f1",
   "metadata": {},
   "source": [
    "This for the data set which is downloaded from online. from the website \"https://www.cs.cmu.edu/~keystroke/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0309472-c630-4e57-8d3a-41379df2fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the merged dataset\n",
    "merged_df = pd.read_csv('DSL-StrongPasswordData.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = merged_df.drop(columns=['subject', 'sessionIndex', 'rep'])  # Features\n",
    "y = merged_df['subject']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e4b22d-e545-4f84-8c1e-0dc3161cfaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Frame dimensions: (20400, 34)\n",
      "X_train dimensions: (20400, 31)\n",
      "y_train dimensions: (20400,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Frame dimensions:\", merged_df.shape)\n",
    "print(\"X_train dimensions:\", X.shape)\n",
    "print(\"y_train dimensions:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ab447b-1f0d-4740-a8df-e93b9f08c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2eb4e1-bbf3-49a0-90d2-20454cdb0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle the training and test sets\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e012d3-03db-4bea-92d0-1ddbe833172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dimensions: (14280, 31)\n",
      "y_train dimensions: (14280,)\n",
      "X_test dimensions: (6120, 31)\n",
      "y_test dimensions: (6120,)\n"
     ]
    }
   ],
   "source": [
    "# Print dimensions of the datasets\n",
    "print(\"X_train dimensions:\", X_train.shape)\n",
    "print(\"y_train dimensions:\", y_train.shape)\n",
    "print(\"X_test dimensions:\", X_test.shape)\n",
    "print(\"y_test dimensions:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bcaf9d-c60f-47f0-943a-4c28a1ff4970",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50497ccf-131c-40f1-93e6-026a87e09f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "235f74df-c130-415f-ab6d-925f417eebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
    "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "train_confusion = confusion_matrix(y_train, y_train_pred)\n",
    "train_classification_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "test_confusion = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_report = classification_report(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8263460-8abc-411f-8e71-3fca125195b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Confusion Matrix:\n",
      " [[280   0   0 ...   0   0   0]\n",
      " [  0 280   0 ...   0   0   0]\n",
      " [  0   0 280 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 280   0   0]\n",
      " [  0   0   0 ...   0 280   0]\n",
      " [  0   0   0 ...   0   0 280]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        s002       1.00      1.00      1.00       280\n",
      "        s003       1.00      1.00      1.00       280\n",
      "        s004       1.00      1.00      1.00       280\n",
      "        s005       1.00      1.00      1.00       280\n",
      "        s007       1.00      1.00      1.00       280\n",
      "        s008       1.00      1.00      1.00       280\n",
      "        s010       1.00      1.00      1.00       280\n",
      "        s011       1.00      1.00      1.00       280\n",
      "        s012       1.00      1.00      1.00       280\n",
      "        s013       1.00      1.00      1.00       280\n",
      "        s015       1.00      1.00      1.00       280\n",
      "        s016       1.00      1.00      1.00       280\n",
      "        s017       1.00      1.00      1.00       280\n",
      "        s018       1.00      1.00      1.00       280\n",
      "        s019       1.00      1.00      1.00       280\n",
      "        s020       1.00      1.00      1.00       280\n",
      "        s021       1.00      1.00      1.00       280\n",
      "        s022       1.00      1.00      1.00       280\n",
      "        s024       1.00      1.00      1.00       280\n",
      "        s025       1.00      1.00      1.00       280\n",
      "        s026       1.00      1.00      1.00       280\n",
      "        s027       1.00      1.00      1.00       280\n",
      "        s028       1.00      1.00      1.00       280\n",
      "        s029       1.00      1.00      1.00       280\n",
      "        s030       1.00      1.00      1.00       280\n",
      "        s031       1.00      1.00      1.00       280\n",
      "        s032       1.00      1.00      1.00       280\n",
      "        s033       1.00      1.00      1.00       280\n",
      "        s034       1.00      1.00      1.00       280\n",
      "        s035       1.00      1.00      1.00       280\n",
      "        s036       1.00      1.00      1.00       280\n",
      "        s037       1.00      1.00      1.00       280\n",
      "        s038       1.00      1.00      1.00       280\n",
      "        s039       1.00      1.00      1.00       280\n",
      "        s040       1.00      1.00      1.00       280\n",
      "        s041       1.00      1.00      1.00       280\n",
      "        s042       1.00      1.00      1.00       280\n",
      "        s043       1.00      1.00      1.00       280\n",
      "        s044       1.00      1.00      1.00       280\n",
      "        s046       1.00      1.00      1.00       280\n",
      "        s047       1.00      1.00      1.00       280\n",
      "        s048       1.00      1.00      1.00       280\n",
      "        s049       1.00      1.00      1.00       280\n",
      "        s050       1.00      1.00      1.00       280\n",
      "        s051       1.00      1.00      1.00       280\n",
      "        s052       1.00      1.00      1.00       280\n",
      "        s053       1.00      1.00      1.00       280\n",
      "        s054       1.00      1.00      1.00       280\n",
      "        s055       1.00      1.00      1.00       280\n",
      "        s056       1.00      1.00      1.00       280\n",
      "        s057       1.00      1.00      1.00       280\n",
      "\n",
      "    accuracy                           1.00     14280\n",
      "   macro avg       1.00      1.00      1.00     14280\n",
      "weighted avg       1.00      1.00      1.00     14280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print training metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1-Score: {train_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", train_confusion)\n",
    "print(\"Classification Report:\\n\", train_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567d8d48-7184-4343-8a40-f4e9e74ea9b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics:\n",
      "Accuracy: 0.9292\n",
      "Precision: 0.9302\n",
      "Recall: 0.9292\n",
      "F1-Score: 0.9287\n",
      "Confusion Matrix:\n",
      " [[ 97   0   0 ...   0   0   0]\n",
      " [  0 109   1 ...   0   0   0]\n",
      " [  0   0 108 ...   0   0   0]\n",
      " ...\n",
      " [  1   0   0 ... 115   2   0]\n",
      " [  0   0   0 ...   0 109   1]\n",
      " [  0   0   0 ...   0   0  97]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        s002       0.82      0.81      0.82       120\n",
      "        s003       0.96      0.91      0.94       120\n",
      "        s004       0.90      0.90      0.90       120\n",
      "        s005       0.90      0.95      0.93       120\n",
      "        s007       0.88      0.82      0.85       120\n",
      "        s008       0.91      0.89      0.90       120\n",
      "        s010       0.94      0.97      0.96       120\n",
      "        s011       0.97      0.87      0.92       120\n",
      "        s012       0.93      0.93      0.93       120\n",
      "        s013       0.97      0.95      0.96       120\n",
      "        s015       0.95      0.90      0.92       120\n",
      "        s016       0.90      0.98      0.94       120\n",
      "        s017       0.97      0.98      0.98       120\n",
      "        s018       0.94      0.80      0.86       120\n",
      "        s019       0.98      0.97      0.97       120\n",
      "        s020       0.94      0.82      0.87       120\n",
      "        s021       0.83      0.94      0.88       120\n",
      "        s022       1.00      1.00      1.00       120\n",
      "        s024       0.96      0.99      0.98       120\n",
      "        s025       0.90      1.00      0.94       120\n",
      "        s026       0.89      0.97      0.93       120\n",
      "        s027       0.93      1.00      0.96       120\n",
      "        s028       0.96      0.99      0.98       120\n",
      "        s029       0.91      0.87      0.89       120\n",
      "        s030       0.90      0.94      0.92       120\n",
      "        s031       0.86      0.90      0.88       120\n",
      "        s032       0.91      0.75      0.82       120\n",
      "        s033       0.98      0.98      0.98       120\n",
      "        s034       0.86      0.85      0.86       120\n",
      "        s035       0.94      0.97      0.96       120\n",
      "        s036       1.00      1.00      1.00       120\n",
      "        s037       0.84      0.90      0.87       120\n",
      "        s038       0.90      0.95      0.92       120\n",
      "        s039       0.95      0.97      0.96       120\n",
      "        s040       0.97      0.97      0.97       120\n",
      "        s041       0.89      0.96      0.92       120\n",
      "        s042       0.92      0.97      0.95       120\n",
      "        s043       1.00      1.00      1.00       120\n",
      "        s044       0.98      0.98      0.98       120\n",
      "        s046       0.94      0.92      0.93       120\n",
      "        s047       0.91      0.95      0.93       120\n",
      "        s048       0.91      0.89      0.90       120\n",
      "        s049       0.90      0.98      0.94       120\n",
      "        s050       0.90      0.93      0.91       120\n",
      "        s051       0.95      0.88      0.92       120\n",
      "        s052       0.99      0.99      0.99       120\n",
      "        s053       0.99      0.97      0.98       120\n",
      "        s054       0.91      0.88      0.89       120\n",
      "        s055       1.00      0.96      0.98       120\n",
      "        s056       0.94      0.91      0.92       120\n",
      "        s057       0.93      0.81      0.87       120\n",
      "\n",
      "    accuracy                           0.93      6120\n",
      "   macro avg       0.93      0.93      0.93      6120\n",
      "weighted avg       0.93      0.93      0.93      6120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print test metrics\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1-Score: {test_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", test_confusion)\n",
    "print(\"Classification Report:\\n\", test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380f221-3bb7-45be-86bf-5a8ca0e8cd3c",
   "metadata": {},
   "source": [
    "import joblib\n",
    "\n",
    "Save the model \n",
    "joblib.dump(rf_model, 'rf_modeldata.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c93db3fa-ab23-43bd-9bc9-b6124091d247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "324 fits failed out of a total of 972.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "201 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "123 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\satvi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.92359944 0.92787115 0.92906162\n",
      " 0.92191877 0.92577031 0.92745098 0.91610644 0.92051821 0.92121849\n",
      " 0.92303922 0.92436975 0.9245098  0.9179972  0.92135854 0.92310924\n",
      " 0.91393557 0.91743697 0.9197479  0.91435574 0.91652661 0.91764706\n",
      " 0.91435574 0.91652661 0.91764706 0.91162465 0.91421569 0.91484594\n",
      " 0.92436975 0.92822129 0.93039216 0.92359944 0.92689076 0.92815126\n",
      " 0.92037815 0.92170868 0.92387955 0.92268908 0.92598039 0.92766106\n",
      " 0.92338936 0.92654062 0.92787115 0.917507   0.92093838 0.92128852\n",
      " 0.91533613 0.9192577  0.9192577  0.91533613 0.9192577  0.9192577\n",
      " 0.91631653 0.91848739 0.9202381         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.86568627 0.86988796 0.87142857 0.86785714 0.87121849 0.87114846\n",
      " 0.86519608 0.86792717 0.86911765 0.86722689 0.86995798 0.87093838\n",
      " 0.86239496 0.86939776 0.86988796 0.86547619 0.86673669 0.8679972\n",
      " 0.86008403 0.86435574 0.86582633 0.86008403 0.86435574 0.86582633\n",
      " 0.86568627 0.86897759 0.86729692 0.87542017 0.88137255 0.88179272\n",
      " 0.87787115 0.8802521  0.87955182 0.87086835 0.87794118 0.87829132\n",
      " 0.8745098  0.88095238 0.87934174 0.8745098  0.8785014  0.87955182\n",
      " 0.87436975 0.87794118 0.87934174 0.8719888  0.87717087 0.87885154\n",
      " 0.8719888  0.87717087 0.87885154 0.87233894 0.8780112  0.8789916\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.92478992 0.92787115 0.92717087\n",
      " 0.92184874 0.92521008 0.92507003 0.91582633 0.9197479  0.92065826\n",
      " 0.92065826 0.92359944 0.92577031 0.9197479  0.92268908 0.92394958\n",
      " 0.91526611 0.91841737 0.91981793 0.91267507 0.91484594 0.91645658\n",
      " 0.91267507 0.91484594 0.91645658 0.90959384 0.91463585 0.91561625\n",
      " 0.92247899 0.92808123 0.9297619  0.92345938 0.92654062 0.92927171\n",
      " 0.92044818 0.92086835 0.92170868 0.92002801 0.9254902  0.92605042\n",
      " 0.92128852 0.92640056 0.92731092 0.91631653 0.92128852 0.92177871\n",
      " 0.91435574 0.91778711 0.92065826 0.91435574 0.91778711 0.92065826\n",
      " 0.91463585 0.91848739 0.9202381         nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.92359944 0.92731092 0.92934174 0.92184874 0.92570028 0.92717087\n",
      " 0.91610644 0.92051821 0.92121849 0.92331933 0.92436975 0.92436975\n",
      " 0.9179972  0.92135854 0.92331933 0.91393557 0.91743697 0.9197479\n",
      " 0.91435574 0.91652661 0.91764706 0.91435574 0.91652661 0.91764706\n",
      " 0.91162465 0.91421569 0.91484594 0.92352941 0.92906162 0.9302521\n",
      " 0.92366947 0.92661064 0.92815126 0.92037815 0.92170868 0.92387955\n",
      " 0.92268908 0.92598039 0.92766106 0.92352941 0.92654062 0.92787115\n",
      " 0.917507   0.92093838 0.92156863 0.91533613 0.9192577  0.9192577\n",
      " 0.91533613 0.9192577  0.9192577  0.91631653 0.91848739 0.9202381 ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import shuffle\n",
    "import joblib\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), \n",
    "                           param_grid=param_grid, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred, average='macro')\n",
    "train_recall = recall_score(y_train, y_train_pred, average='macro')\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "train_confusion = confusion_matrix(y_train, y_train_pred)\n",
    "train_classification_report = classification_report(y_train, y_train_pred)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "test_recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "test_confusion = confusion_matrix(y_test, y_test_pred)\n",
    "test_classification_report = classification_report(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d372d6cb-f14c-4020-aaa8-a32a97cd56dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "Confusion Matrix:\n",
      " [[280   0   0 ...   0   0   0]\n",
      " [  0 280   0 ...   0   0   0]\n",
      " [  0   0 280 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 280   0   0]\n",
      " [  0   0   0 ...   0 280   0]\n",
      " [  0   0   0 ...   0   0 280]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        s002       1.00      1.00      1.00       280\n",
      "        s003       1.00      1.00      1.00       280\n",
      "        s004       1.00      1.00      1.00       280\n",
      "        s005       1.00      1.00      1.00       280\n",
      "        s007       1.00      1.00      1.00       280\n",
      "        s008       1.00      1.00      1.00       280\n",
      "        s010       1.00      1.00      1.00       280\n",
      "        s011       1.00      1.00      1.00       280\n",
      "        s012       1.00      1.00      1.00       280\n",
      "        s013       1.00      1.00      1.00       280\n",
      "        s015       1.00      1.00      1.00       280\n",
      "        s016       1.00      1.00      1.00       280\n",
      "        s017       1.00      1.00      1.00       280\n",
      "        s018       1.00      1.00      1.00       280\n",
      "        s019       1.00      1.00      1.00       280\n",
      "        s020       1.00      1.00      1.00       280\n",
      "        s021       1.00      1.00      1.00       280\n",
      "        s022       1.00      1.00      1.00       280\n",
      "        s024       1.00      1.00      1.00       280\n",
      "        s025       1.00      1.00      1.00       280\n",
      "        s026       1.00      1.00      1.00       280\n",
      "        s027       1.00      1.00      1.00       280\n",
      "        s028       1.00      1.00      1.00       280\n",
      "        s029       1.00      1.00      1.00       280\n",
      "        s030       1.00      1.00      1.00       280\n",
      "        s031       1.00      1.00      1.00       280\n",
      "        s032       1.00      1.00      1.00       280\n",
      "        s033       1.00      1.00      1.00       280\n",
      "        s034       1.00      1.00      1.00       280\n",
      "        s035       1.00      1.00      1.00       280\n",
      "        s036       1.00      1.00      1.00       280\n",
      "        s037       1.00      1.00      1.00       280\n",
      "        s038       1.00      1.00      1.00       280\n",
      "        s039       1.00      1.00      1.00       280\n",
      "        s040       1.00      1.00      1.00       280\n",
      "        s041       1.00      1.00      1.00       280\n",
      "        s042       1.00      1.00      1.00       280\n",
      "        s043       1.00      1.00      1.00       280\n",
      "        s044       1.00      1.00      1.00       280\n",
      "        s046       1.00      1.00      1.00       280\n",
      "        s047       1.00      1.00      1.00       280\n",
      "        s048       1.00      1.00      1.00       280\n",
      "        s049       1.00      1.00      1.00       280\n",
      "        s050       1.00      1.00      1.00       280\n",
      "        s051       1.00      1.00      1.00       280\n",
      "        s052       1.00      1.00      1.00       280\n",
      "        s053       1.00      1.00      1.00       280\n",
      "        s054       1.00      1.00      1.00       280\n",
      "        s055       1.00      1.00      1.00       280\n",
      "        s056       1.00      1.00      1.00       280\n",
      "        s057       1.00      1.00      1.00       280\n",
      "\n",
      "    accuracy                           1.00     14280\n",
      "   macro avg       1.00      1.00      1.00     14280\n",
      "weighted avg       1.00      1.00      1.00     14280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print training metrics\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Precision: {train_precision:.4f}\")\n",
    "print(f\"Recall: {train_recall:.4f}\")\n",
    "print(f\"F1-Score: {train_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", train_confusion)\n",
    "print(\"Classification Report:\\n\", train_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2003d118-9919-443d-a41d-e31a04cac7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Metrics:\n",
      "Accuracy: 0.9351\n",
      "Precision: 0.9363\n",
      "Recall: 0.9351\n",
      "F1-Score: 0.9346\n",
      "Confusion Matrix:\n",
      " [[ 95   0   0 ...   0   0   0]\n",
      " [  0 110   2 ...   0   0   0]\n",
      " [  0   0 108 ...   0   0   0]\n",
      " ...\n",
      " [  2   0   0 ... 115   2   0]\n",
      " [  0   0   0 ...   0 109   0]\n",
      " [  0   0   0 ...   0   0 102]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        s002       0.83      0.79      0.81       120\n",
      "        s003       0.96      0.92      0.94       120\n",
      "        s004       0.88      0.90      0.89       120\n",
      "        s005       0.93      0.95      0.94       120\n",
      "        s007       0.88      0.88      0.88       120\n",
      "        s008       0.95      0.88      0.92       120\n",
      "        s010       0.94      0.98      0.96       120\n",
      "        s011       0.98      0.88      0.93       120\n",
      "        s012       0.90      0.95      0.93       120\n",
      "        s013       0.98      0.97      0.97       120\n",
      "        s015       0.93      0.91      0.92       120\n",
      "        s016       0.90      0.97      0.93       120\n",
      "        s017       0.99      0.99      0.99       120\n",
      "        s018       0.97      0.79      0.87       120\n",
      "        s019       0.99      0.97      0.98       120\n",
      "        s020       0.96      0.80      0.87       120\n",
      "        s021       0.85      0.96      0.90       120\n",
      "        s022       0.99      1.00      1.00       120\n",
      "        s024       0.97      0.99      0.98       120\n",
      "        s025       0.91      1.00      0.95       120\n",
      "        s026       0.91      0.97      0.94       120\n",
      "        s027       0.94      1.00      0.97       120\n",
      "        s028       0.98      1.00      0.99       120\n",
      "        s029       0.91      0.88      0.90       120\n",
      "        s030       0.91      0.95      0.93       120\n",
      "        s031       0.84      0.91      0.87       120\n",
      "        s032       0.91      0.75      0.82       120\n",
      "        s033       0.98      0.99      0.98       120\n",
      "        s034       0.94      0.85      0.89       120\n",
      "        s035       0.96      0.97      0.97       120\n",
      "        s036       0.99      1.00      1.00       120\n",
      "        s037       0.85      0.91      0.88       120\n",
      "        s038       0.91      0.96      0.93       120\n",
      "        s039       0.94      0.97      0.96       120\n",
      "        s040       0.95      0.98      0.97       120\n",
      "        s041       0.88      0.97      0.92       120\n",
      "        s042       0.96      0.97      0.96       120\n",
      "        s043       1.00      1.00      1.00       120\n",
      "        s044       0.95      0.98      0.97       120\n",
      "        s046       0.90      0.91      0.90       120\n",
      "        s047       0.89      0.96      0.92       120\n",
      "        s048       0.92      0.91      0.91       120\n",
      "        s049       0.94      0.98      0.96       120\n",
      "        s050       0.90      0.93      0.92       120\n",
      "        s051       0.96      0.92      0.94       120\n",
      "        s052       0.99      1.00      1.00       120\n",
      "        s053       0.99      0.97      0.98       120\n",
      "        s054       0.94      0.88      0.91       120\n",
      "        s055       1.00      0.96      0.98       120\n",
      "        s056       0.96      0.91      0.94       120\n",
      "        s057       0.94      0.85      0.89       120\n",
      "\n",
      "    accuracy                           0.94      6120\n",
      "   macro avg       0.94      0.94      0.93      6120\n",
      "weighted avg       0.94      0.94      0.93      6120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print test metrics\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1-Score: {test_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", test_confusion)\n",
    "print(\"Classification Report:\\n\", test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccb473-ebb9-4b3a-ba39-3820e319158d",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1d5aa50-4386-4248-8661-65f2cd5216b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_neighbors': 5}\n",
      "\n",
      "Training Metrics (Best Model):\n",
      "Accuracy: 0.8245\n",
      "Precision: 0.8368\n",
      "Recall: 0.8245\n",
      "F1-Score: 0.8251\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        s002       0.51      0.85      0.64       280\n",
      "        s003       0.79      0.89      0.84       280\n",
      "        s004       0.68      0.82      0.74       280\n",
      "        s005       0.77      0.92      0.84       280\n",
      "        s007       0.75      0.82      0.79       280\n",
      "        s008       0.75      0.80      0.78       280\n",
      "        s010       0.84      0.93      0.88       280\n",
      "        s011       0.94      0.85      0.90       280\n",
      "        s012       0.91      0.87      0.89       280\n",
      "        s013       0.82      0.84      0.83       280\n",
      "        s015       0.82      0.78      0.80       280\n",
      "        s016       0.62      0.90      0.74       280\n",
      "        s017       0.88      0.95      0.91       280\n",
      "        s018       0.89      0.82      0.86       280\n",
      "        s019       0.86      0.92      0.89       280\n",
      "        s020       0.85      0.64      0.73       280\n",
      "        s021       0.80      0.85      0.83       280\n",
      "        s022       0.77      0.94      0.85       280\n",
      "        s024       0.74      0.88      0.80       280\n",
      "        s025       0.73      0.94      0.83       280\n",
      "        s026       0.69      0.80      0.74       280\n",
      "        s027       0.85      0.87      0.86       280\n",
      "        s028       0.87      0.89      0.88       280\n",
      "        s029       0.80      0.86      0.83       280\n",
      "        s030       0.81      0.86      0.84       280\n",
      "        s031       0.70      0.65      0.68       280\n",
      "        s032       0.88      0.54      0.67       280\n",
      "        s033       0.83      0.81      0.82       280\n",
      "        s034       0.90      0.71      0.79       280\n",
      "        s035       0.88      0.73      0.80       280\n",
      "        s036       0.89      0.95      0.92       280\n",
      "        s037       0.84      0.73      0.78       280\n",
      "        s038       0.85      0.86      0.85       280\n",
      "        s039       0.91      0.82      0.86       280\n",
      "        s040       0.91      0.74      0.81       280\n",
      "        s041       0.92      0.78      0.84       280\n",
      "        s042       0.96      0.91      0.93       280\n",
      "        s043       0.90      0.90      0.90       280\n",
      "        s044       0.84      0.85      0.85       280\n",
      "        s046       0.83      0.76      0.79       280\n",
      "        s047       0.79      0.77      0.78       280\n",
      "        s048       0.84      0.80      0.82       280\n",
      "        s049       0.82      0.80      0.81       280\n",
      "        s050       0.82      0.69      0.75       280\n",
      "        s051       0.94      0.78      0.85       280\n",
      "        s052       0.95      0.96      0.96       280\n",
      "        s053       0.98      0.90      0.94       280\n",
      "        s054       0.89      0.84      0.87       280\n",
      "        s055       0.99      0.91      0.95       280\n",
      "        s056       0.94      0.75      0.84       280\n",
      "        s057       0.91      0.63      0.74       280\n",
      "\n",
      "    accuracy                           0.82     14280\n",
      "   macro avg       0.84      0.82      0.83     14280\n",
      "weighted avg       0.84      0.82      0.83     14280\n",
      "\n",
      "\n",
      "Test Metrics (Best Model):\n",
      "Accuracy: 0.7333\n",
      "Precision: 0.7503\n",
      "Recall: 0.7333\n",
      "F1-Score: 0.7331\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        s002       0.41      0.72      0.52       120\n",
      "        s003       0.67      0.72      0.70       120\n",
      "        s004       0.49      0.60      0.54       120\n",
      "        s005       0.69      0.80      0.74       120\n",
      "        s007       0.66      0.81      0.73       120\n",
      "        s008       0.66      0.76      0.71       120\n",
      "        s010       0.86      0.89      0.87       120\n",
      "        s011       0.88      0.68      0.77       120\n",
      "        s012       0.82      0.76      0.79       120\n",
      "        s013       0.68      0.79      0.73       120\n",
      "        s015       0.73      0.62      0.67       120\n",
      "        s016       0.53      0.82      0.64       120\n",
      "        s017       0.80      0.92      0.86       120\n",
      "        s018       0.81      0.62      0.70       120\n",
      "        s019       0.82      0.89      0.85       120\n",
      "        s020       0.70      0.50      0.58       120\n",
      "        s021       0.70      0.79      0.75       120\n",
      "        s022       0.72      0.91      0.80       120\n",
      "        s024       0.68      0.80      0.73       120\n",
      "        s025       0.72      0.88      0.79       120\n",
      "        s026       0.54      0.77      0.64       120\n",
      "        s027       0.74      0.87      0.80       120\n",
      "        s028       0.80      0.87      0.83       120\n",
      "        s029       0.68      0.70      0.69       120\n",
      "        s030       0.73      0.81      0.77       120\n",
      "        s031       0.52      0.48      0.50       120\n",
      "        s032       0.77      0.41      0.53       120\n",
      "        s033       0.69      0.66      0.67       120\n",
      "        s034       0.81      0.66      0.72       120\n",
      "        s035       0.82      0.59      0.69       120\n",
      "        s036       0.88      0.93      0.91       120\n",
      "        s037       0.68      0.57      0.62       120\n",
      "        s038       0.72      0.73      0.72       120\n",
      "        s039       0.78      0.78      0.78       120\n",
      "        s040       0.81      0.53      0.64       120\n",
      "        s041       0.83      0.69      0.75       120\n",
      "        s042       0.95      0.93      0.94       120\n",
      "        s043       0.78      0.87      0.82       120\n",
      "        s044       0.78      0.81      0.80       120\n",
      "        s046       0.72      0.60      0.65       120\n",
      "        s047       0.69      0.64      0.66       120\n",
      "        s048       0.74      0.72      0.73       120\n",
      "        s049       0.71      0.68      0.69       120\n",
      "        s050       0.75      0.66      0.70       120\n",
      "        s051       0.85      0.59      0.70       120\n",
      "        s052       0.93      0.94      0.94       120\n",
      "        s053       0.95      0.82      0.88       120\n",
      "        s054       0.80      0.75      0.78       120\n",
      "        s055       0.99      0.86      0.92       120\n",
      "        s056       0.92      0.68      0.78       120\n",
      "        s057       0.91      0.53      0.67       120\n",
      "\n",
      "    accuracy                           0.73      6120\n",
      "   macro avg       0.75      0.73      0.73      6120\n",
      "weighted avg       0.75      0.73      0.73      6120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9, 11, 13, 15,17,19,21,25]}\n",
    "\n",
    "# Initialize the KNN model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Save the best model\n",
    "best_knn = grid_search.best_estimator_\n",
    "# joblib.dump(best_knn, 'knn_best.joblib')\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_train_pred_best = best_knn.predict(X_train)\n",
    "y_test_pred_best = best_knn.predict(X_test)\n",
    "\n",
    "# Calculate metrics for training set (best model)\n",
    "train_accuracy_best = accuracy_score(y_train, y_train_pred_best)\n",
    "train_precision_best = precision_score(y_train, y_train_pred_best, average='macro')\n",
    "train_recall_best = recall_score(y_train, y_train_pred_best, average='macro')\n",
    "train_f1_best = f1_score(y_train, y_train_pred_best, average='macro')\n",
    "# train_confusion_best = confusion_matrix(y_train, y_train_pred_best)\n",
    "train_classification_report_best = classification_report(y_train, y_train_pred_best)\n",
    "\n",
    "# Calculate metrics for test set (best model)\n",
    "test_accuracy_best = accuracy_score(y_test, y_test_pred_best)\n",
    "test_precision_best = precision_score(y_test, y_test_pred_best, average='macro')\n",
    "test_recall_best = recall_score(y_test, y_test_pred_best, average='macro')\n",
    "test_f1_best = f1_score(y_test, y_test_pred_best, average='macro')\n",
    "# test_confusion_best = confusion_matrix(y_test, y_test_pred_best)\n",
    "test_classification_report_best = classification_report(y_test, y_test_pred_best)\n",
    "\n",
    "# Print best parameters and metrics\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"\\nTraining Metrics (Best Model):\")\n",
    "print(f\"Accuracy: {train_accuracy_best:.4f}\")\n",
    "print(f\"Precision: {train_precision_best:.4f}\")\n",
    "print(f\"Recall: {train_recall_best:.4f}\")\n",
    "print(f\"F1-Score: {train_f1_best:.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", train_confusion_best)\n",
    "print(\"Classification Report:\\n\", train_classification_report_best)\n",
    "\n",
    "print(\"\\nTest Metrics (Best Model):\")\n",
    "print(f\"Accuracy: {test_accuracy_best:.4f}\")\n",
    "print(f\"Precision: {test_precision_best:.4f}\")\n",
    "print(f\"Recall: {test_recall_best:.4f}\")\n",
    "print(f\"F1-Score: {test_f1_best:.4f}\")\n",
    "# print(\"Confusion Matrix:\\n\", test_confusion_best)\n",
    "print(\"Classification Report:\\n\", test_classification_report_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990ac18-a3b7-45d6-958d-0ee5f7e36476",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6d16b0-f08e-430f-8556-d6f410fd436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for SVM: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "\n",
      "Training Metrics (Best SVM Model):\n",
      "Accuracy: 0.9912\n",
      "Precision: 0.9913\n",
      "Recall: 0.9912\n",
      "F1-Score: 0.9912\n",
      "Confusion Matrix:\n",
      " [[276   0   0 ...   0   0   0]\n",
      " [  0 278   1 ...   0   0   0]\n",
      " [  2   1 271 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 276   3   0]\n",
      " [  0   0   0 ...   2 276   0]\n",
      " [  0   0   0 ...   0   0 274]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        s002       0.95      0.99      0.97       280\n",
      "        s003       0.98      0.99      0.99       280\n",
      "        s004       0.99      0.97      0.98       280\n",
      "        s005       1.00      1.00      1.00       280\n",
      "        s007       0.96      0.96      0.96       280\n",
      "        s008       0.99      0.96      0.97       280\n",
      "        s010       0.99      1.00      0.99       280\n",
      "        s011       0.99      0.99      0.99       280\n",
      "        s012       1.00      0.99      0.99       280\n",
      "        s013       1.00      0.99      0.99       280\n",
      "        s015       0.99      0.99      0.99       280\n",
      "        s016       1.00      1.00      1.00       280\n",
      "        s017       1.00      0.99      1.00       280\n",
      "        s018       1.00      0.99      0.99       280\n",
      "        s019       1.00      1.00      1.00       280\n",
      "        s020       1.00      0.99      0.99       280\n",
      "        s021       0.98      0.99      0.98       280\n",
      "        s022       1.00      1.00      1.00       280\n",
      "        s024       1.00      1.00      1.00       280\n",
      "        s025       0.99      1.00      0.99       280\n",
      "        s026       0.99      0.99      0.99       280\n",
      "        s027       0.98      1.00      0.99       280\n",
      "        s028       1.00      1.00      1.00       280\n",
      "        s029       0.97      0.97      0.97       280\n",
      "        s030       1.00      1.00      1.00       280\n",
      "        s031       1.00      0.99      0.99       280\n",
      "        s032       0.98      0.94      0.96       280\n",
      "        s033       1.00      1.00      1.00       280\n",
      "        s034       0.99      1.00      1.00       280\n",
      "        s035       0.99      1.00      0.99       280\n",
      "        s036       1.00      1.00      1.00       280\n",
      "        s037       0.98      0.99      0.99       280\n",
      "        s038       1.00      1.00      1.00       280\n",
      "        s039       1.00      1.00      1.00       280\n",
      "        s040       1.00      1.00      1.00       280\n",
      "        s041       1.00      1.00      1.00       280\n",
      "        s042       1.00      1.00      1.00       280\n",
      "        s043       1.00      1.00      1.00       280\n",
      "        s044       1.00      1.00      1.00       280\n",
      "        s046       1.00      1.00      1.00       280\n",
      "        s047       0.99      1.00      1.00       280\n",
      "        s048       0.98      0.98      0.98       280\n",
      "        s049       1.00      1.00      1.00       280\n",
      "        s050       1.00      0.99      0.99       280\n",
      "        s051       0.99      0.98      0.99       280\n",
      "        s052       1.00      1.00      1.00       280\n",
      "        s053       1.00      1.00      1.00       280\n",
      "        s054       0.98      0.99      0.99       280\n",
      "        s055       0.99      0.99      0.99       280\n",
      "        s056       0.98      0.99      0.98       280\n",
      "        s057       0.99      0.98      0.99       280\n",
      "\n",
      "    accuracy                           0.99     14280\n",
      "   macro avg       0.99      0.99      0.99     14280\n",
      "weighted avg       0.99      0.99      0.99     14280\n",
      "\n",
      "\n",
      "Test Metrics (Best SVM Model):\n",
      "Accuracy: 0.8765\n",
      "Precision: 0.8791\n",
      "Recall: 0.8765\n",
      "F1-Score: 0.8765\n",
      "Confusion Matrix:\n",
      " [[ 98   2   1 ...   0   1   0]\n",
      " [  0 109   1 ...   0   0   0]\n",
      " [  2   6  95 ...   0   0   0]\n",
      " ...\n",
      " [  1   0   0 ... 110   3   0]\n",
      " [  0   0   0 ...   1 104   1]\n",
      " [  0   0   0 ...   0   0  96]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        s002       0.77      0.82      0.79       120\n",
      "        s003       0.87      0.91      0.89       120\n",
      "        s004       0.90      0.79      0.84       120\n",
      "        s005       0.96      0.88      0.92       120\n",
      "        s007       0.79      0.88      0.83       120\n",
      "        s008       0.84      0.91      0.87       120\n",
      "        s010       0.90      0.95      0.92       120\n",
      "        s011       0.91      0.82      0.86       120\n",
      "        s012       0.90      0.92      0.91       120\n",
      "        s013       0.84      0.91      0.88       120\n",
      "        s015       0.87      0.85      0.86       120\n",
      "        s016       0.80      0.90      0.85       120\n",
      "        s017       0.97      0.95      0.96       120\n",
      "        s018       0.91      0.86      0.88       120\n",
      "        s019       0.93      0.93      0.93       120\n",
      "        s020       0.86      0.79      0.82       120\n",
      "        s021       0.81      0.88      0.84       120\n",
      "        s022       0.87      0.94      0.90       120\n",
      "        s024       0.90      0.93      0.91       120\n",
      "        s025       0.89      0.92      0.90       120\n",
      "        s026       0.78      0.88      0.83       120\n",
      "        s027       0.92      0.91      0.92       120\n",
      "        s028       0.95      0.97      0.96       120\n",
      "        s029       0.86      0.79      0.83       120\n",
      "        s030       0.88      0.93      0.90       120\n",
      "        s031       0.74      0.75      0.74       120\n",
      "        s032       0.81      0.73      0.77       120\n",
      "        s033       0.89      0.93      0.91       120\n",
      "        s034       0.88      0.80      0.84       120\n",
      "        s035       0.90      0.93      0.91       120\n",
      "        s036       0.94      0.86      0.90       120\n",
      "        s037       0.80      0.80      0.80       120\n",
      "        s038       0.83      0.86      0.84       120\n",
      "        s039       0.92      0.91      0.92       120\n",
      "        s040       0.89      0.91      0.90       120\n",
      "        s041       0.91      0.89      0.90       120\n",
      "        s042       0.92      0.97      0.95       120\n",
      "        s043       0.94      0.93      0.94       120\n",
      "        s044       0.82      0.93      0.87       120\n",
      "        s046       0.87      0.79      0.83       120\n",
      "        s047       0.86      0.82      0.84       120\n",
      "        s048       0.92      0.82      0.87       120\n",
      "        s049       0.72      0.91      0.80       120\n",
      "        s050       0.88      0.88      0.88       120\n",
      "        s051       0.89      0.79      0.84       120\n",
      "        s052       1.00      0.97      0.99       120\n",
      "        s053       0.97      0.93      0.95       120\n",
      "        s054       0.85      0.79      0.82       120\n",
      "        s055       0.99      0.92      0.95       120\n",
      "        s056       0.91      0.87      0.89       120\n",
      "        s057       0.90      0.80      0.85       120\n",
      "\n",
      "    accuracy                           0.88      6120\n",
      "   macro avg       0.88      0.88      0.88      6120\n",
      "weighted avg       0.88      0.88      0.88      6120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import joblib\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel types\n",
    "    'gamma': ['scale', 'auto']  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# # Save the best SVM model\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "# joblib.dump(best_svm, 'svm_best.joblib')\n",
    "\n",
    "# Make predictions using the best SVM model\n",
    "y_train_pred_best_svm = best_svm.predict(X_train)\n",
    "y_test_pred_best_svm = best_svm.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the training set (best SVM model)\n",
    "train_accuracy_best_svm = accuracy_score(y_train, y_train_pred_best_svm)\n",
    "train_precision_best_svm = precision_score(y_train, y_train_pred_best_svm, average='macro')\n",
    "train_recall_best_svm = recall_score(y_train, y_train_pred_best_svm, average='macro')\n",
    "train_f1_best_svm = f1_score(y_train, y_train_pred_best_svm, average='macro')\n",
    "train_confusion_best_svm = confusion_matrix(y_train, y_train_pred_best_svm)\n",
    "train_classification_report_best_svm = classification_report(y_train, y_train_pred_best_svm)\n",
    "\n",
    "# Calculate metrics for the test set (best SVM model)\n",
    "test_accuracy_best_svm = accuracy_score(y_test, y_test_pred_best_svm)\n",
    "test_precision_best_svm = precision_score(y_test, y_test_pred_best_svm, average='macro')\n",
    "test_recall_best_svm = recall_score(y_test, y_test_pred_best_svm, average='macro')\n",
    "test_f1_best_svm = f1_score(y_test, y_test_pred_best_svm, average='macro')\n",
    "test_confusion_best_svm = confusion_matrix(y_test, y_test_pred_best_svm)\n",
    "test_classification_report_best_svm = classification_report(y_test, y_test_pred_best_svm)\n",
    "\n",
    "# Print best parameters and metrics\n",
    "print(\"Best Parameters for SVM:\", grid_search_svm.best_params_)\n",
    "print(\"\\nTraining Metrics (Best SVM Model):\")\n",
    "print(f\"Accuracy: {train_accuracy_best_svm:.4f}\")\n",
    "print(f\"Precision: {train_precision_best_svm:.4f}\")\n",
    "print(f\"Recall: {train_recall_best_svm:.4f}\")\n",
    "print(f\"F1-Score: {train_f1_best_svm:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", train_confusion_best_svm)\n",
    "print(\"Classification Report:\\n\", train_classification_report_best_svm)\n",
    "\n",
    "print(\"\\nTest Metrics (Best SVM Model):\")\n",
    "print(f\"Accuracy: {test_accuracy_best_svm:.4f}\")\n",
    "print(f\"Precision: {test_precision_best_svm:.4f}\")\n",
    "print(f\"Recall: {test_recall_best_svm:.4f}\")\n",
    "print(f\"F1-Score: {test_f1_best_svm:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", test_confusion_best_svm)\n",
    "print(\"Classification Report:\\n\", test_classification_report_best_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67cc9224-aea7-4b6f-ba12-9dcf17d07560",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d532ef13-ee72-401c-b925-e17bcae6b07d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 100, 'solver': 'liblinear'}\n",
      "\n",
      "Training Metrics (Best Model):\n",
      "Accuracy: 0.8267\n",
      "Precision: 0.8271\n",
      "Recall: 0.8266\n",
      "F1-Score: 0.8247\n",
      "Confusion Matrix:\n",
      " [[232   6   2 ...   0   1   0]\n",
      " [  7 258  14 ...   0   1   0]\n",
      " [  6   7 259 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ... 309   6   3]\n",
      " [  5   0   2 ...   3 259   5]\n",
      " [  0   0   0 ...   0   6 215]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70       328\n",
      "           1       0.83      0.78      0.80       331\n",
      "           2       0.80      0.79      0.80       327\n",
      "           3       0.83      0.86      0.85       327\n",
      "           4       0.81      0.66      0.73       346\n",
      "           5       0.78      0.70      0.74       313\n",
      "           6       0.89      0.90      0.90       303\n",
      "           7       0.86      0.83      0.85       325\n",
      "           8       0.86      0.76      0.81       307\n",
      "           9       0.85      0.84      0.84       328\n",
      "          10       0.82      0.64      0.71       313\n",
      "          11       0.73      0.91      0.81       315\n",
      "          12       0.94      0.92      0.93       318\n",
      "          13       0.81      0.80      0.81       327\n",
      "          14       0.84      0.94      0.88       320\n",
      "          15       0.74      0.64      0.69       314\n",
      "          16       0.75      0.79      0.77       314\n",
      "          17       0.97      0.96      0.96       328\n",
      "          18       0.91      0.93      0.92       327\n",
      "          19       0.79      0.95      0.86       315\n",
      "          20       0.77      0.80      0.79       307\n",
      "          21       0.81      0.92      0.86       317\n",
      "          22       0.89      0.90      0.89       315\n",
      "          23       0.78      0.86      0.82       324\n",
      "          24       0.83      0.92      0.87       313\n",
      "          25       0.69      0.64      0.66       309\n",
      "          26       0.70      0.47      0.56       323\n",
      "          27       0.93      0.93      0.93       331\n",
      "          28       0.84      0.72      0.78       333\n",
      "          29       0.88      0.87      0.87       319\n",
      "          30       0.97      0.97      0.97       320\n",
      "          31       0.74      0.70      0.72       322\n",
      "          32       0.78      0.87      0.82       319\n",
      "          33       0.87      0.90      0.88       323\n",
      "          34       0.82      0.91      0.86       315\n",
      "          35       0.86      0.77      0.81       304\n",
      "          36       0.93      0.93      0.93       321\n",
      "          37       0.97      0.97      0.97       323\n",
      "          38       0.83      0.85      0.84       319\n",
      "          39       0.73      0.80      0.76       311\n",
      "          40       0.71      0.79      0.75       329\n",
      "          41       0.73      0.85      0.79       327\n",
      "          42       0.85      0.89      0.87       319\n",
      "          43       0.74      0.78      0.76       311\n",
      "          44       0.74      0.75      0.75       325\n",
      "          45       0.99      0.99      0.99       312\n",
      "          46       0.97      0.95      0.96       328\n",
      "          47       0.68      0.71      0.70       317\n",
      "          48       0.97      0.92      0.95       335\n",
      "          49       0.84      0.82      0.83       316\n",
      "          50       0.80      0.70      0.75       307\n",
      "\n",
      "    accuracy                           0.83     16320\n",
      "   macro avg       0.83      0.83      0.82     16320\n",
      "weighted avg       0.83      0.83      0.82     16320\n",
      "\n",
      "\n",
      "Test Metrics (Best Model):\n",
      "Accuracy: 0.8314\n",
      "Precision: 0.8328\n",
      "Recall: 0.8317\n",
      "F1-Score: 0.8292\n",
      "Confusion Matrix:\n",
      " [[53  2  3 ...  0  0  0]\n",
      " [ 0 48  2 ...  0  0  0]\n",
      " [ 2  1 62 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 60  0  1]\n",
      " [ 0  0  0 ...  1 70  0]\n",
      " [ 0  0  1 ...  0  0 66]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.74      0.71        72\n",
      "           1       0.75      0.70      0.72        69\n",
      "           2       0.78      0.85      0.82        73\n",
      "           3       0.85      0.86      0.86        73\n",
      "           4       0.67      0.61      0.64        54\n",
      "           5       0.84      0.72      0.78        87\n",
      "           6       0.91      0.88      0.89        97\n",
      "           7       0.83      0.76      0.79        75\n",
      "           8       0.89      0.73      0.80        93\n",
      "           9       0.85      0.88      0.86        72\n",
      "          10       0.82      0.64      0.72        87\n",
      "          11       0.74      0.95      0.84        85\n",
      "          12       0.93      0.93      0.93        82\n",
      "          13       0.80      0.77      0.78        73\n",
      "          14       0.79      0.95      0.86        80\n",
      "          15       0.74      0.69      0.71        86\n",
      "          16       0.73      0.81      0.77        86\n",
      "          17       0.96      1.00      0.98        72\n",
      "          18       0.84      0.95      0.89        73\n",
      "          19       0.83      0.93      0.88        85\n",
      "          20       0.80      0.84      0.82        93\n",
      "          21       0.79      0.93      0.85        83\n",
      "          22       0.95      0.94      0.95        85\n",
      "          23       0.91      0.82      0.86        76\n",
      "          24       0.83      0.99      0.91        87\n",
      "          25       0.77      0.56      0.65        91\n",
      "          26       0.63      0.42      0.50        77\n",
      "          27       0.94      0.96      0.95        69\n",
      "          28       0.87      0.79      0.83        67\n",
      "          29       0.94      0.89      0.91        81\n",
      "          30       1.00      0.97      0.99        80\n",
      "          31       0.83      0.73      0.78        78\n",
      "          32       0.85      0.88      0.86        81\n",
      "          33       0.86      0.91      0.89        77\n",
      "          34       0.86      0.92      0.89        85\n",
      "          35       0.86      0.74      0.79        96\n",
      "          36       0.95      0.96      0.96        79\n",
      "          37       0.97      0.99      0.98        77\n",
      "          38       0.90      0.88      0.89        81\n",
      "          39       0.77      0.81      0.79        89\n",
      "          40       0.65      0.77      0.71        71\n",
      "          41       0.62      0.82      0.71        73\n",
      "          42       0.84      0.88      0.86        81\n",
      "          43       0.73      0.81      0.77        89\n",
      "          44       0.82      0.83      0.82        75\n",
      "          45       1.00      0.93      0.96        88\n",
      "          46       0.96      0.94      0.95        72\n",
      "          47       0.66      0.72      0.69        83\n",
      "          48       0.91      0.92      0.92        65\n",
      "          49       0.91      0.83      0.87        84\n",
      "          50       0.85      0.71      0.77        93\n",
      "\n",
      "    accuracy                           0.83      4080\n",
      "   macro avg       0.83      0.83      0.83      4080\n",
      "weighted avg       0.83      0.83      0.83      4080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels as integers if they are not already\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga']  # 'liblinear' is good for small datasets, 'saga' can handle large datasets\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search_log_reg = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_log_reg = grid_search_log_reg.best_estimator_\n",
    "\n",
    "# Make predictions on the training set using the best model\n",
    "y_train_pred_best = best_log_reg.predict(X_train)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_test_pred_best = best_log_reg.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "train_accuracy_best = accuracy_score(y_train, y_train_pred_best)\n",
    "train_precision_best = precision_score(y_train, y_train_pred_best, average='macro')\n",
    "train_recall_best = recall_score(y_train, y_train_pred_best, average='macro')\n",
    "train_f1_best = f1_score(y_train, y_train_pred_best, average='macro')\n",
    "train_confusion_best = confusion_matrix(y_train, y_train_pred_best)\n",
    "train_classification_report_best = classification_report(y_train, y_train_pred_best)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "test_accuracy_best = accuracy_score(y_test, y_test_pred_best)\n",
    "test_precision_best = precision_score(y_test, y_test_pred_best, average='macro')\n",
    "test_recall_best = recall_score(y_test, y_test_pred_best, average='macro')\n",
    "test_f1_best = f1_score(y_test, y_test_pred_best, average='macro')\n",
    "test_confusion_best = confusion_matrix(y_test, y_test_pred_best)\n",
    "test_classification_report_best = classification_report(y_test, y_test_pred_best)\n",
    "\n",
    "# Print best parameters and metrics\n",
    "print(\"Best Parameters:\", grid_search_log_reg.best_params_)\n",
    "print(\"\\nTraining Metrics (Best Model):\")\n",
    "print(f\"Accuracy: {train_accuracy_best:.4f}\")\n",
    "print(f\"Precision: {train_precision_best:.4f}\")\n",
    "print(f\"Recall: {train_recall_best:.4f}\")\n",
    "print(f\"F1-Score: {train_f1_best:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", train_confusion_best)\n",
    "print(\"Classification Report:\\n\", train_classification_report_best)\n",
    "\n",
    "print(\"\\nTest Metrics (Best Model):\")\n",
    "print(f\"Accuracy: {test_accuracy_best:.4f}\")\n",
    "print(f\"Precision: {test_precision_best:.4f}\")\n",
    "print(f\"Recall: {test_recall_best:.4f}\")\n",
    "print(f\"F1-Score: {test_f1_best:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", test_confusion_best)\n",
    "print(\"Classification Report:\\n\", test_classification_report_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56f186-69fb-4e76-8aa4-80a4bf80e910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
